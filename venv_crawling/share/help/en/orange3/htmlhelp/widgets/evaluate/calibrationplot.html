
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=cp1252" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Calibration Plot</title>
    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/alabaster.css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="next" title="Confusion Matrix" href="confusionmatrix.html" />
    <link rel="prev" title="Save Model" href="../model/savemodel.html" />
   
  <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="calibration-plot">
<h1>Calibration Plot</h1>
<p>Shows the match between classifiers’ probability predictions and actual class probabilities.</p>
<p><strong>Inputs</strong></p>
<ul class="simple">
<li>Evaluation Results: results of testing classification algorithms</li>
</ul>
<p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Calibration_curve" target="_blank">Calibration Plot</a>plots class probabilities against those predicted by the classifier(s).</p>
<p><img alt="../../_images/CalibrationPlot-stamped.png" src="../../_images/CalibrationPlot-stamped.png" /></p>
<ol class="simple">
<li>Select the desired target class from the drop down menu.</li>
<li>Choose which classifiers to plot. The diagonal represents optimal behavior; the closer the classifier’s curve gets, the more accurate its prediction probabilities are. Thus we would use this widget to see whether a classifier is overly optimistic (gives predominantly positive results) or pessimistic (gives predominantly negative results).</li>
<li>If <em>Show rug</em> is enabled, ticks are displayed at the bottom and the top of the graph, which represent negative and positive examples respectively. Their position corresponds to the classifier’s probability prediction and the color shows the classifier. At the bottom of the graph, the points to the left are those which are (correctly) assigned a low probability of the target class, and those to the right are incorrectly assigned high probabilities. At the top of the graph, the instances to the right are correctly assigned high probabilities and vice versa.</li>
<li>Press <em>Save Image</em> if you want to save the created image to your computer in a .svg or .png format.</li>
<li>Produce a report.</li>
</ol>
<div class="section" id="example">
<h2>Example</h2>
<p>At the moment, the only widget which gives the right type of signal needed by the <strong>Calibration Plot</strong> is <a class="reference internal" href="testandscore.html"><span class="doc">Test &amp; Score</span></a>. The Calibration Plot will hence always follow Test &amp; Score and, since it has no outputs, no other widgets follow it.</p>
<p>Here is a typical example, where we compare three classifiers (namely <a class="reference internal" href="../model/naivebayes.html"><span class="doc">Naive Bayes</span></a>, <a class="reference internal" href="../model/tree.html"><span class="doc">Tree</span></a> and <a class="reference internal" href="../model/constant.html"><span class="doc">Constant</span></a>) and input them into <a class="reference internal" href="testandscore.html"><span class="doc">Test &amp; Score</span></a>. We used the <em>Titanic</em> dataset. Test &amp; Score then displays evaluation results for each classifier. Then we draw <strong>Calibration Plot</strong> and <a class="reference internal" href="rocanalysis.html"><span class="doc">ROC Analysis</span></a> widgets from Test &amp; Score to further analyze the performance of classifiers. <strong>Calibration Plot</strong> enables you to see prediction accuracy of class probabilities in a plot.</p>
<p><img alt="../../_images/CalibrationPlot-example.png" src="../../_images/CalibrationPlot-example.png" /></p>
</div>
</div>


          </div>
          
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2015, Orange Data Mining.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.5.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../../_sources/widgets/evaluate/calibrationplot.md.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>